# -*- coding: utf-8 -*-
"""vq_file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m55nEbYRBmod17vojkNHe2L1q1lkBqL_
"""



import torch
import numpy as np
from sklearn.cluster import KMeans


# Configuration: specify the target layer and number of clusters
layer_name = 'fc3'       # Layer to quantize (e.g., 'fc3'; change this to quantize other layers)
num_clusters = 8         # Number of K-means clusters (e.g., 8 clusters -> 3-bit quantization)


# Load the pre-trained model and its state dict
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net()                  # instantiate your model class (must be defined/imported)
model.load_state_dict(torch.load('model_cifar.pth'))  # load the saved weights
model.eval()  # set to evaluation mode, just as good practice (not strictly necessary for clustering)

# Extract the weight tensor of the target layer
weight_tensor = getattr(model, layer_name).weight.data  # PyTorch tensor of the weights
weight_shape = weight_tensor.shape                      # original shape (for later reconstruction)
weight_values = weight_tensor.cpu().numpy().ravel()     # flatten to 1D NumPy array for clustering

print(f"Quantizing layer '{layer_name}' with weight shape {weight_shape} containing {weight_values.size} values.")

# Apply K-means clustering to the weight values
kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=0)  # KMeans configuration
weight_values_2d = weight_values.reshape(-1, 1)  # reshape to (num_weights, 1) for fitting
kmeans.fit(weight_values_2d)

# Retrieve the cluster centers and labels for each weight
centroids = kmeans.cluster_centers_.flatten().astype(np.float32)
labels = kmeans.labels_  # this is a 1D array of length = number of weights

# Prepare data for saving
indices = labels.astype(np.uint8)  # cast labels to uint8 for compact storage (if clusters <= 256)
original_shape = np.array(weight_shape)  # convert shape to numpy array (to store in npz)

# Save the centroids, indices, and shape to a .npz file
output_npz = f"quantized_{layer_name}.npz"
np.savez(output_npz, centroids=centroids, indices=indices, shape=original_shape)
print(f"Quantization data saved to {output_npz}")

# (Later) Reconstruct the weight tensor from the .npz file
data = np.load(output_npz)
centroids_loaded = data['centroids']        # array of centroids (float32)
indices_loaded = data['indices']            # array of indices (uint8)
original_shape_loaded = tuple(data['shape'])  # original shape of the weight tensor

# Reconstruct the weight values using the centroids and indices
# Each entry in indices_loaded is an index into the centroids array.
reconstructed_weights = centroids_loaded[indices_loaded.astype(np.int64)]
reconstructed_weights = reconstructed_weights.reshape(original_shape_loaded)

# Convert the reconstructed weight array back to a torch tensor
quantized_weight_tensor = torch.tensor(reconstructed_weights, dtype=weight_tensor.dtype)
quantized_weight_tensor = quantized_weight_tensor.to(weight_tensor.device)

# Replace the model's layer weight with this quantized weight
with torch.no_grad():
    getattr(model, layer_name).weight.copy_(quantized_weight_tensor)

# (Optional) If the layer has a bias, you might also consider whether to quantize or keep the bias.
# Here we leave the bias unchanged.

# Save the updated model (state dict) to a new file
torch.save(model.state_dict(), "cifar_net_quantized.pth")
print("Quantized model saved as cifar_net_quantized.pth")

quantized_model = Net()
quantized_model.load_state_dict(torch.load("cifar_net_quantized.pth"))
quantized_model.eval()
# Now use quantized_model for inference...

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
import os

# Define the common CNN architecture for CIFAR-10
class MyCNNModel(nn.Module):
    def __init__(self):
        super(MyCNNModel, self).__init__()
        # Two convolutional layers
        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)   # 3 input channels, 6 output channels, 5x5 conv
        self.pool = nn.MaxPool2d(2, 2)               # 2x2 max pooling
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # 6 input channels, 16 output channels, 5x5 conv
        # Three fully-connected layers
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 16*5*5 inputs (flattened) -> 120 outputs
        self.fc2 = nn.Linear(120, 84)         # 120 inputs -> 84 outputs
        self.fc3 = nn.Linear(84, 10)          # 84 inputs -> 10 outputs (one for each class)
    def forward(self, x):
        # Convolutional layers with ReLU and pooling
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        # Flatten the tensor before fully-connected layers
        x = torch.flatten(x, 1)  # flatten all dimensions except batch
        # Fully-connected layers with ReLU (except final output layer)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize two model instances (same architecture)
model_fp = MyCNNModel()      # original full-precision model
model_quant = MyCNNModel()   # quantized model (will load quantized weights)

# Load the trained weights for each model
model_fp.load_state_dict(torch.load("model_cifar.pth", map_location=torch.device('cpu')))
model_quant.load_state_dict(torch.load("cifar_net_quantized.pth", map_location=torch.device('cpu')))

# Set both models to evaluation mode
model_fp.eval()
model_quant.eval()

#Determine file sizes (in KB) for each model file
file_size_fp = os.path.getsize("model_cifar.pth") / 1024.0
file_size_quant = os.path.getsize("cifar_net_quantized.pth") / 1024.0

# Calculate in-memory size of parameters (and buffers) for each model (in bytes)
param_size_fp = sum(p.nelement() * p.element_size() for p in model_fp.parameters())
buffer_size_fp = sum(b.nelement() * b.element_size() for b in model_fp.buffers())
total_size_fp_bytes = param_size_fp + buffer_size_fp

param_size_quant = sum(p.nelement() * p.element_size() for p in model_quant.parameters())
buffer_size_quant = sum(b.nelement() * b.element_size() for b in model_quant.buffers())
total_size_quant_bytes = param_size_quant + buffer_size_quant

# Convert byte sizes to MB for readability
total_size_fp_mb = total_size_fp_bytes / (1024.0 ** 2)
total_size_quant_mb = total_size_quant_bytes / (1024.0 ** 2)

# Print file size and memory size comparisons
print(f"Original model file size: {file_size_fp:.2f} KB")
print(f"Quantized model file size: {file_size_quant:.2f} KB")
print(f"Original model in-memory size: {total_size_fp_mb:.2f} MB")
print(f"Quantized model in-memory size: {total_size_quant_mb:.2f} MB")

# Define the standard CIFAR-10 normalization and load the test dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])
testset = torchvision.datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)
testloader = DataLoader(testset, batch_size=100, shuffle=False)

# Evaluate both models on the CIFAR-10 test set to calculate accuracy
correct_fp = 0
correct_quant = 0
total = 0
with torch.no_grad():
    for images, labels in testloader:
        # Using CPU for inference; if GPU is available and models are on GPU, move images to GPU here
        outputs_fp = model_fp(images)
        outputs_quant = model_quant(images)
        # Predicted class is the index of the max logit
        _, pred_fp = torch.max(outputs_fp, 1)
        _, pred_quant = torch.max(outputs_quant, 1)
        # Update totals
        total += labels.size(0)
        correct_fp += (pred_fp == labels).sum().item()
        correct_quant += (pred_quant == labels).sum().item()

accuracy_fp = 100.0 * correct_fp / total
accuracy_quant = 100.0 * correct_quant / total

# Print accuracy results
print(f"Original model test accuracy: {accuracy_fp:.2f}%")
print(f"Quantized model test accuracy: {accuracy_quant:.2f}%")

# Class names for CIFAR-10 classes
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# Get a small batch of test images for sample predictions (e.g., 5 images)
dataiter = iter(testloader)
images, labels = next(dataiter)
images_sample = images[:5]
labels_sample = labels[:5]

# Get predictions from both models on the sample
outputs_fp_sample = model_fp(images_sample)
outputs_quant_sample = model_quant(images_sample)
_, preds_fp = torch.max(outputs_fp_sample, 1)
_, preds_quant = torch.max(outputs_quant_sample, 1)

# Print predicted and true class names for the sample images
print("\nSample predictions (first 5 test images):")
for i in range(len(images_sample)):
    true_label = classes[labels_sample[i].item()]
    pred_label_fp = classes[preds_fp[i].item()]
    pred_label_quant = classes[preds_quant[i].item()]
    print(f"Image {i+1}: True = {true_label}, Original Pred = {pred_label_fp}, Quantized Pred = {pred_label_quant}")